 ## Data Engineer
USA, | Jan 2025- Present

Responsibilities:
‚Ä¢	Build and maintain robust ETL pipelines to move and transform data from various sources into an enterprise data warehouse for reporting and analytics.
‚Ä¢	Built data ingestion pipelines using Fabric Data Pipelines and Dataflows Gen2, integrating structured and unstructured data into Fabric Lakehouse for reporting and AI workloads.
‚Ä¢	Designed and optimized data models, schemas, and T-SQL queries; performed performance tuning and indexing for Azure SQL and on-prem databases.
‚Ä¢	Developed and maintained ETL pipelines using Azure Data Factory (ADF), SQL Server Integration Services (SSIS), and SQL Server to validate, transform, and load data into enterprise data warehouses.
‚Ä¢	Built interactive Power BI dashboards and Azure Analysis Services tabular models with advanced DAX measures and row-level security (RLS) for secure, role-based analytics.
‚Ä¢	Migrated legacy reporting tools (Argos, Crystal Reports) to SQL Server Reporting Services (SSRS) in Azure, delivering parameterized, graphical, and subscription-enabled reports.
‚Ä¢	Automated workflows, including pipeline runs, report refreshes, and cube processing, using ADF triggers, Airflow DAGs and SQL Server Agent schedules.
‚Ä¢	Integrated AI-powered classification and NLP models for metadata enrichment, automated tagging, sentiment analysis, and compliance validation.
Implemented role-based access control (RBAC) and enforced security best practices across ETL, analytics, & reporting platforms
 
 


‚Ä¢	Utilized Apache Spark for distributed data processing and batch transformations, enabling scalable pipeline execution on large datasets.
‚Ä¢	Conducted data ingestion and storage using Hadoop HDFS and integrated Apache Hive for efficient querying and schema management.
‚Ä¢	Built and deployed serverless ETL pipelines using AWS Lambda, integrating with AWS S3 and AWS RDS for cloud-native data flow management.
‚Ä¢	Designed and maintained ETL workflows using Talend to automate data extraction, transformation, and loading processes across diverse data sources.
‚Ä¢	Wrote optimized SQL queries for data validation, transformation, and aggregation tasks, improving data processing efficiency and integrity.
‚Ä¢	Developed Python-based scripts leveraging Pandas and NumPy for advanced data cleaning, manipulation, and exploratory data analysis (EDA).
‚Ä¢	Implemented real-time data ingestion and transformation workflows in AWS EC2, ensuring high availability and low-latency data access.
‚Ä¢	Automated recurring data pipeline tasks with workflow orchestration tools, reducing manual effort and improving reliability.
‚Ä¢	Monitored end-to-end data pipeline performance, identifying and tuning bottlenecks to enhance throughput and system stability.
‚Ä¢	Conducted detailed data validation and quality checks to ensure accuracy and completeness of processed datasets.
‚Ä¢	Applied data cleaning and transformation techniques to prepare datasets for reporting, machine learning, and analytics use cases.
‚Ä¢	Designed scalable data ingestion pipelines to collect, filter, and store structured and semi-structured data from multiple sources.
‚Ä¢	Executed exploratory data analysis (EDA) to uncover trends and anomalies, supporting business decision-making and model development.
‚Ä¢	Followed best practices in performance tuning ETL processes, Spark jobs, and SQL queries to meet SLA and cost-efficiency targets.
Created interactive reports using Excel Pivot Tables to summarize and visualize key metrics for business stakeholders.
Environment: Apache Spark, Hadoop HDFS, Apache Hive, AWS Lambda, AWS S3, AWS RDS, Talend, SQL, Python, Pandas, NumPy, AWS EC2, Workflow Orchestration Tools, Excel (Pivot Tables). 


‚Ä¢	Partnered with BI and analytics teams to design a centralized Azure data lakehouse for clinical product performance and operational analytics.
‚Ä¢	Designed fact/dimension models, stored procedures, and lookup tables to support regulatory reporting and predictive analytics.
‚Ä¢	Built ETL pipelines in Azure Databricks using PySpark and Delta Lake with Medallion architecture (Bronze, Silver, Gold) for clean, curated datasets.
‚Ä¢	Developed custom PySpark ML pipelines for predictive forecasting and inventory optimization, and integrated Azure OpenAI Service for automated report summarization and natural language insights.
‚Ä¢	Created transformation logic in Azure Synapse with T-SQL, improving dashboard query performance.
‚Ä¢	Integrated real-time streaming pipelines from hospital systems and inventory feeds via Azure Event Hubs and Stream Analytics for near real-time insights
‚Ä¢	Provided production support and root cause analysis for critical jobs, improving SLA adherence and system reliability.


## Python 

Developed Python-based data processing and automation scripts using Pandas, NumPy, and standard libraries to clean, transform, and analyze structured datasets.

Built backend services and utilities in Python for data ingestion, validation, and API integration.

Implemented modular, reusable Python code following OOP principles, improving maintainability and scalability.

Used Python for AI/ML and NLP pipelines, including preprocessing, embeddings generation, and inference workflows.

Debugged and optimized Python scripts to improve performance and reduce execution time.

## Java 

Designed and implemented Java-based applications using object-oriented programming concepts such as inheritance, encapsulation, and polymorphism.

Developed backend logic and utility classes in Java for data handling and business logic processing.

Worked with Java collections, exception handling, and multithreading concepts to build robust applications.

Integrated Java applications with external APIs and databases using structured and maintainable code practices.

Followed clean code principles and version control workflows while developing Java projects.

## ü§ñ AI (Artificial Intelligence) 

Built and experimented with AI-driven applications leveraging Large Language Models (LLMs) for reasoning, text generation, and automation tasks.

Designed AI workflows for decision-making, data interpretation, and user interaction.

Integrated AI models into full-stack applications, enabling intelligent features such as chat, summarization, and recommendation.

Evaluated AI outputs using prompt engineering, structured outputs, and response validation techniques.

Applied AI concepts such as retrieval, inference, and feedback loops to real-world use cases.

## üß† Agentic AI 

Designed and implemented Agentic AI systems capable of autonomous task execution, planning, and tool usage.

Built AI agents that break down complex tasks into subtasks, reason step-by-step, and execute actions dynamically.

Integrated agents with external tools such as APIs, databases, and local scripts to perform real-world operations.

Implemented multi-agent workflows for coordination, delegation, and result aggregation.

Applied agentic patterns including ReAct, tool-calling, memory management, and feedback loops.

Developed and tested agent pipelines using frameworks and custom logic for scalable agent orchestration.

üó£Ô∏è NLP (Natural Language Processing) ‚Äì Experience Points

Built NLP pipelines for text preprocessing, tokenization, normalization, and semantic analysis.

Implemented text-based features such as classification, summarization, keyword extraction, and sentiment analysis.

Worked with embeddings and vector representations to enable semantic search and retrieval-augmented generation (RAG).

Integrated NLP models into applications for conversational AI and intelligent text interfaces.

Evaluated NLP outputs for accuracy, relevance, and contextual understanding.
